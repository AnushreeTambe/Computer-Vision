{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import cv2\n",
    "import numpy as np\n",
    "from model.yolo_model import YOLO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_image(img):\n",
    "    '''Resize, reduce and expand image.\n",
    "    \n",
    "    # Argument :\n",
    "        img : original image\n",
    "        \n",
    "    # Returns : \n",
    "        image : original image\n",
    "    '''\n",
    "    \n",
    "    image = cv2.resize(img, (416, 416), interpolation=cv2.INTER_CUBIC)\n",
    "    image = np.array(image, dtype='float32')\n",
    "    image /= 255\n",
    "    image = np.expand_dims(image, axis=0)\n",
    "    \n",
    "    return image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function takes an original image as input and performs several processing steps on it.\n",
    "\n",
    "Resize: The cv2.resize() function resizes the input image to a specified size (416, 416). \n",
    "It uses the cv2. INTER_CUBIC interpolation method, which is a higher-quality interpolation method compared to others like cv2.INTER_LINEAR.\n",
    "\n",
    "Data type conversion: The resized image is then converted to a NumPy array with dtype='float32'. This step ensures that the image data type is suitable for further processing, typically in machine learning or computer vision tasks.\n",
    "\n",
    "Normalization:  The pixel values of the image are normalized by dividing each pixel value by 255. This step scales the pixel values to the range [0,1], which is a common practice for neural network input data normalization\n",
    "\n",
    "Dimension expansion: Finally, np.expand_dims() is used to add an extra d imension to the processed image array. This is often necessary when deal ing with batch processing in deep learning frameworks, where the first d imension represents the batch size. In this case, axis-e indicates that the new dimension is added as the first dimension.\n",
    "\n",
    "The function returns the processed image with the specified shape (1, 416, 416, 3), where 1 represents the batch size, 416 and 416 represent the image dimensions, and 3 represents the number of color channels (RGB)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_classes(file):\n",
    "    '''\n",
    "    Get classes name\n",
    "    \n",
    "    # Argument:\n",
    "        file : classes name for database.\n",
    "        \n",
    "    # Returns:\n",
    "        class_names : List, classes name.\n",
    "    '''\n",
    "    \n",
    "    with open(file) as f:\n",
    "        class_names = f.readlines()\n",
    "    class_names = [c.strip() for c in class_names]\n",
    "    \n",
    "    return class_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function is designed to read a file containing class names and return them as a list.\n",
    "\n",
    "Opening the File: The function takes a parameter file, which is the path to the file containing the class names. It then opens this file using a with statement, which ensures that the file is properly closed after its suite finishes, even if an exception is raised.\n",
    "\n",
    "Reading Class Names: Inside the with block, f.readlines() reads all the lines from the file and returns them as a list of strings. Each string represents a class name.\n",
    "\n",
    "Stripping Newlines: Since readlines() includes the newline character \\n at the end of each line, the list of class names may contain trailing wh itespace. The list comprehension [c.strip() for c in class_names] is used to remove leading and trailing whitespace (including newlines) from each class name.\n",
    "\n",
    "Returning Class Names: The function then returns the list of class names.\n",
    "\n",
    "Overall, this function is a simple utility for extracting class names from a file. It's commonly used in machine learning and computer vision tasks where class names are stored externally, such as in a text file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "This function, draw(image, boxes, scores, classes, all_classes), is intended to draw bounding boxes around detected objects \n",
    "on an image along with their corresponding class labels and confidence scores.\n",
    "'''\n",
    "def draw(image, boxes, scores, classes, all_classes):\n",
    "    '''\n",
    "    Draw the boxes on the image.\n",
    "    \n",
    "    # Argument : \n",
    "        image: original image\n",
    "        boxes: ndarray, boxes of objects\n",
    "        classes: ndarray, classes of objects\n",
    "        scores: ndarray, scores of objects\n",
    "        all_classes: all classes name\n",
    "    '''\n",
    "    for box, score, cl in zip(boxes, scores, classes):\n",
    "        x, y, w, h = box\n",
    "        \n",
    "        top = max(0, np.floor(x + 0.5).astype(int))\n",
    "        left = max(0, np.floor(x + 0.5).astype(int))\n",
    "        right = min(image.shape[1], np.floor(x + w + 0.5).astype(int))\n",
    "        bottom = min(image.shape[0], np.floor(y + h + 0.5).astype(int))\n",
    "        \n",
    "        cv2.rectangle(image, (top, left), (right, bottom), (255, 0, 0), 2)\n",
    "        cv2.putText(image, '{0} {1:.2f}'.format(all_classes[cl], score),\n",
    "                    (top, left -6),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                    0.6, (0, 0, 255), 1,\n",
    "                    cv2.LINE_AA\n",
    "                   )\n",
    "        \n",
    "        print('class: {0}, score: {1:.2f}'.format(all_classes[cl], score))\n",
    "        print('box coordinate x,y,w,h: {0}'.format(box))\n",
    "        \n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function is designed to draw bounding boxes around detected objects on an image, along with their class labels and confidence scores.\n",
    "\n",
    "    Iterating Over Detected Objects:\n",
    "    It iterates over each detected object using a for loop and zip(boxe s, scores, classes), where boxes, scores, and classes are arrays containing the bounding box coordinates, confidence scores, and class indices o f detected objects, respectively.\n",
    "\n",
    "    Bounding Box Coordinates:\n",
    "    For each detected object, it unpacks the bounding box coordinates (x, y, w, h) from box.\n",
    "    It calculates the top-left (top, left) and bottom-right (right, bott on) coordinates of the bounding box. These coordinates are used to draw the rectangle\n",
    "\n",
    "    Drawing Bounding Boxes:\n",
    "    Using cv2.rectangle(), it draws a rectangle around the detected obje ct on the image. The rectangle is drawn using coordinates (top, left) an d (right, bottom) with a blue color (255, 0, 0) and a thickness of 2 pix els.\n",
    "\n",
    "    Annotating with Class Label and Score:\n",
    "    It annotates the bounding box with the class label and confidence sc ore using cv2.putText(). The label and score are formatted with the clas s name obtained from all classes [cl] (where cl is the class index) and t he confidence score. This annotation is placed slightly above the top-le ft corner of the bounding box with a red color (0, 0, 255) and a font scale of 0.6.\n",
    "\n",
    "    Printing Information:\n",
    "    For each detected object, it prints the class name and confidence score along with the bounding box coordinates.\n",
    "\n",
    "    Blank Line:\n",
    "    Finally, it prints a blank line to separate the output for different images if multiple images are being processed.\n",
    "\n",
    "    This function is commonly used in object detection tasks to visualize the detected objects on an image for validation or debugging purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_image(image, yolo, all_classes):\n",
    "    '''\n",
    "    Use YOLO v3 to detect images.\n",
    "    \n",
    "    # Argument:\n",
    "        image: original image\n",
    "        yolo: YOLO, yolo model\n",
    "        all_classes: all classes names\n",
    "        \n",
    "    # Returns:\n",
    "        image: processed image\n",
    "    '''\n",
    "    \n",
    "    pimage = process_image(image)\n",
    "    \n",
    "    #The image is first processed into a format that the YOLO model \n",
    "    #expects, typically involving resizing, normalizing, and possibly \n",
    "    #transforming the image to fit the input shape for the YOLO model.\n",
    "    \n",
    "    start = time.time()\n",
    "    boxes, classes, scores = yolo.predict(pimage, image.shape)\n",
    "    end = time.time()\n",
    "    \n",
    "    print('time: {0:.2f}s'.format(end - start))\n",
    "\n",
    "#Here, the function calls YOLO's predict method on the processed image\n",
    "#(pimage). This method returns three things:\n",
    "\n",
    "#boxes: coordinates of bounding boxes for detected objects.\n",
    "#classes: the class indices (e.g., 'e' for 'person', '1' for \"car\", etc.).\n",
    "#scores: confidence scores indicating how certain the model is that an object\n",
    "#The time taken for the detection is measured using time.time() to calculate\n",
    "    \n",
    "    if boxes is not None:\n",
    "        draw(image, boxes, scores, classes, all_classes)\n",
    "#If YOLD detects any objects (i.e., boxes is not None)\n",
    "#the draw function is called. This function will typically:\n",
    "#Draw bounding boxes on the original image.\n",
    "\n",
    "#Label each box with the class name (using all_classes).\n",
    "#Optionally include the confidence score.\n",
    "\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_video(video, yolo, all_classes):\n",
    "    \"\"\"Use yolo v3 to detect video.\n",
    "\n",
    "    # Argument:\n",
    "        video: video file.\n",
    "        yolo: YOLO, yolo model.\n",
    "        all_classes: all classes name.\n",
    "    \"\"\"\n",
    "    video_path = os.path.join(\"C:/10-Computer_Vision/YOLOv3/videos\", \"C:/10-Computer_Vision/YOLOv3/videos/test\", video)\n",
    "    camera = cv2.VideoCapture(video_path)\n",
    "    cv2.namedWindow(\"C:/10-Computer_Vision/YOLOv3/detection\", cv2.WINDOW_AUTOSIZE)\n",
    "    \n",
    "    #fource= cv2.VideoWriter_fourcc(**mpeg') specifies the codec 28 to be used for saving the video (in this case, the 'moea codec).\n",
    "    #video_path constructs the path to the input video file by joining folder\n",
    "    #names and the video filename.\n",
    "    #camera cv2.VideoCapture(video_path) initializes the video capture object #using OpenCV (cv2). This allows reading frames from the video.\n",
    "    #cv2.namedWindow creates a window named \"detection\" for displaying the video\n",
    "    \n",
    "    # Prepare for saving the detected video\n",
    "    sz = (int(camera.get(cv2.CAP_PROP_FRAME_WIDTH)),\n",
    "        int(camera.get(cv2.CAP_PROP_FRAME_HEIGHT)))\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mpeg')\n",
    "    \n",
    "    #sz stores the dimensions of the video (width and height),\n",
    "    #which are retrieved using OpenCV's camera.get method.\n",
    "    \n",
    "    vout = cv2.VideoWriter()\n",
    "    vout.open(os.path.join(\"C:/10-Computer_Vision/YOLOv3/videos\", \"C:\\10-Computer_Vision\\YOLOv3\\videos\\res\", video), fourcc, 20, sz, True)\n",
    "    \n",
    "    #vout = cv2.VideoWriter() creates a video writer object,\n",
    "    #and vout.open opens a new video file where the processed frames (with object\n",
    "    while True:\n",
    "        res, frame = camera.read()\n",
    "\n",
    "        if not res:\n",
    "            break\n",
    "\n",
    "        image = detect_image(frame, yolo, all_classes)\n",
    "        cv2.imshow(\"C:/10-Computer Vision/YOLOv3/detection\", image)\n",
    "    #The function detect_image(frame, yolo, all_classes) (the one we discussed eai\n",
    "    #The processed frame (with bounding boxes and Labels) is stored in image.\n",
    "    #cv2.imshow(\"detection\", image) displays the frame in the \"detection\" window\n",
    "    \n",
    "        # Save the video frame by frame\n",
    "        vout.write(image)\n",
    "\n",
    "        if cv2.waitKey(110) & 0xff == 27:\n",
    "                break\n",
    "\n",
    "    vout.release()\n",
    "    camera.release()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "yolo = YOLO(0.6, 0.5)\n",
    "file = 'C:/10-Computer_Vision/YOLOv3/data/coco_classes.txt'\n",
    "all_classes = get_classes(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Detecting Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 37.50s\n",
      "class: person, score: 1.00\n",
      "box coordinate x,y,w,h: [187.71986008  84.54503465  91.60767555 304.36179113]\n",
      "class: horse, score: 1.00\n",
      "box coordinate x,y,w,h: [396.45050049 137.2821641  215.70493698 208.54855251]\n",
      "class: dog, score: 1.00\n",
      "box coordinate x,y,w,h: [ 61.28296375 263.38459015 145.58371544  88.16218674]\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f = 'C:/10-Computer_Vision/YOLOv3/images/test/person.jpg'\n",
    "image = cv2.imread('C:/10-Computer_Vision/YOLOv3/images/test/person.jpg')\n",
    "image = detect_image(image, yolo, all_classes)\n",
    "cv2.imwrite('C:/10-Computer_Vision/YOLOv3/images/res' + f, image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
